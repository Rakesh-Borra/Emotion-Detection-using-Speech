{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KnRf-WA3GF49"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wsE02iXFjl-c",
        "outputId": "91730788-2bac-4454-974d-6961b597b328"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 98,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l74j5svXm5nc"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mmg3iqhD8F15"
      },
      "source": [
        "# **Reading the csv**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XrnzzgS8GxMH",
        "outputId": "4da1a46d-c179-4c8a-f2fc-e3709a3c805a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "         tweet_id  ...                                            content\n",
            "0      1956967341  ...  @tiffanylue i know  i was listenin to bad habi...\n",
            "1      1956967666  ...  Layin n bed with a headache  ughhhh...waitin o...\n",
            "2      1956967696  ...                Funeral ceremony...gloomy friday...\n",
            "3      1956967789  ...               wants to hang out with friends SOON!\n",
            "4      1956968416  ...  @dannycastillo We want to trade with someone w...\n",
            "...           ...  ...                                                ...\n",
            "39995  1753918954  ...                                   @JohnLloydTaylor\n",
            "39996  1753919001  ...                     Happy Mothers Day  All my love\n",
            "39997  1753919005  ...  Happy Mother's Day to all the mommies out ther...\n",
            "39998  1753919043  ...  @niariley WASSUP BEAUTIFUL!!! FOLLOW ME!!  PEE...\n",
            "39999  1753919049  ...  @mopedronin bullet train from tokyo    the gf ...\n",
            "\n",
            "[40000 rows x 3 columns]\n"
          ]
        }
      ],
      "source": [
        "data= pd.read_csv(\"/content/tweet_emotions.csv\")\n",
        "print(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iMdSZF20G1yb",
        "outputId": "14c091ae-7e5d-4a80-9bc6-67ce26efd867"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['tweet_id', 'sentiment', 'content'], dtype='object')"
            ]
          },
          "execution_count": 101,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y3DA_qUWHY-z",
        "outputId": "669618f5-6de4-4d78-de53-514dba0e4d83"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'anger', 'fun', 'neutral', 'boredom', 'worry', 'surprise', 'relief', 'love', 'hate', 'empty', 'happiness', 'sadness', 'enthusiasm'}\n"
          ]
        }
      ],
      "source": [
        "y_total = list(data.sentiment)\n",
        "print((set(y_total)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11EJevRZbumE",
        "outputId": "72005f5c-a7c4-4468-df73-bd886fae40f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2187\n",
            "8638\n",
            "5209\n",
            "1526\n",
            "5165\n",
            "8459\n",
            "1776\n",
            "827\n",
            "110\n",
            "3842\n",
            "179\n",
            "759\n",
            "1323\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(len(data[data.sentiment == 'surprise']))\n",
        "print(len(data[data.sentiment == 'neutral']))\n",
        "print(len(data[data.sentiment == 'happiness']))\n",
        "print(len(data[data.sentiment == 'relief']))\n",
        "print(len(data[data.sentiment == 'sadness']))\n",
        "print(len(data[data.sentiment == 'worry']))\n",
        "print(len(data[data.sentiment == 'fun']))\n",
        "print(len(data[data.sentiment == 'empty']))\n",
        "print(len(data[data.sentiment == 'anger']))\n",
        "print(len(data[data.sentiment == 'love']))\n",
        "print(len(data[data.sentiment == 'boredom']))\n",
        "print(len(data[data.sentiment == 'enthusiasm']))\n",
        "print(len(data[data.sentiment == 'hate']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aSMGvRqGSadq"
      },
      "outputs": [],
      "source": [
        "# Dropping rows with other emotion labels\n",
        "data = data.drop(data[data.sentiment == 'anger'].index)\n",
        "data = data.drop(data[data.sentiment == 'enthusiasm'].index)\n",
        "data = data.drop(data[data.sentiment == 'boredom'].index)\n",
        "data = data.drop(data[data.sentiment == 'empty'].index)\n",
        "data = data.drop(data[data.sentiment == 'hate'].index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "chAmlbn2M4o_",
        "outputId": "8b686f04-0d1a-4ee1-c12e-2b2f7b56d803"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'neutral', 'positive', 'negative'}\n"
          ]
        }
      ],
      "source": [
        "data['sentiment'] = data['sentiment'].replace({'anger': 'negative'})\n",
        "data['sentiment'] = data['sentiment'].replace({'fun': 'positive'})\n",
        "data['sentiment'] = data['sentiment'].replace({'boredom': 'negative'})\n",
        "data['sentiment'] = data['sentiment'].replace({'worry': 'negative'})\n",
        "data['sentiment'] = data['sentiment'].replace({'surprise': 'positive'})\n",
        "data['sentiment'] = data['sentiment'].replace({'relief': 'positive'})\n",
        "data['sentiment'] = data['sentiment'].replace({'love': 'positive'})\n",
        "data['sentiment'] = data['sentiment'].replace({'hate': 'negative'})\n",
        "data['sentiment'] = data['sentiment'].replace({'empty': 'neutral'})\n",
        "data['sentiment'] = data['sentiment'].replace({'happiness': 'positive'})\n",
        "data['sentiment'] = data['sentiment'].replace({'sadness': 'negative'})\n",
        "data['sentiment'] = data['sentiment'].replace({'enthusiasm': 'neutral'})\n",
        "\n",
        "  \n",
        "# # writing into the file\n",
        "# df.to_csv(\"AllDetails.csv\", index=False)\n",
        "  \n",
        "y_total = list(data.sentiment)\n",
        "print((set(y_total)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3CmrjJRfG8D"
      },
      "source": [
        "# **Converting text to lowercase**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vyHWTji0e-AJ",
        "outputId": "7b481abf-33bc-47cd-c420-290bb86e1acb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1        layin n bed with a headache ughhhh...waitin on...\n",
            "2                      funeral ceremony...gloomy friday...\n",
            "4        @dannycastillo we want to trade with someone w...\n",
            "5        re-pinging @ghostridah14: why didn't you go to...\n",
            "6        i should be sleep, but im not! thinking about ...\n",
            "                               ...                        \n",
            "39995                                     @johnlloydtaylor\n",
            "39996                        happy mothers day all my love\n",
            "39997    happy mother's day to all the mommies out ther...\n",
            "39998    @niariley wassup beautiful!!! follow me!! peep...\n",
            "39999    @mopedronin bullet train from tokyo the gf and...\n",
            "Name: content, Length: 36802, dtype: object\n"
          ]
        }
      ],
      "source": [
        "data['content'] = data['content'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
        "print(data['content'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTmPRiaffT7H"
      },
      "source": [
        "# **Removing punctuations and symbols**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vbvwtunIfR-K"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "data['content'] = data['content'].apply(lambda x: \" \".join(x.translate(str.maketrans('','',string.punctuation)) for x in x.split()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GeX8Gabah6y6",
        "outputId": "214693e0-f21b-46bd-fd46-1d84be4dc184"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1        layin n bed with a headache ughhhhwaitin on yo...\n",
            "2                            funeral ceremonygloomy friday\n",
            "4        dannycastillo we want to trade with someone wh...\n",
            "5        repinging ghostridah14 why didnt you go to pro...\n",
            "6        i should be sleep but im not thinking about an...\n",
            "                               ...                        \n",
            "39995                                      johnlloydtaylor\n",
            "39996                        happy mothers day all my love\n",
            "39997    happy mothers day to all the mommies out there...\n",
            "39998    niariley wassup beautiful follow me peep out m...\n",
            "39999    mopedronin bullet train from tokyo the gf and ...\n",
            "Name: content, Length: 36802, dtype: object\n"
          ]
        }
      ],
      "source": [
        "print(data['content'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4G0nXjsCiZwf"
      },
      "source": [
        "# **Removing stop words**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rsmQh6GRiZAO"
      },
      "outputs": [],
      "source": [
        "stop_words = [\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\",\n",
        "              \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\",\n",
        "              \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\",\n",
        "              \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\",\n",
        "              \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\",\n",
        "              \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\",\n",
        "              \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\",\n",
        "              \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\",\n",
        "              \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\",\n",
        "              \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"]\n",
        "\n",
        "data['content'] = data['content'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop_words))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KNoZy915ipKQ",
        "outputId": "3b4cebf5-4aca-4ec7-af05-266a18d011b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1                   layin n bed headache ughhhhwaitin call\n",
            "2                            funeral ceremonygloomy friday\n",
            "4        dannycastillo want trade someone houston ticke...\n",
            "5        repinging ghostridah14 didnt go prom bc bf did...\n",
            "6        sleep im thinking old friend want hes married ...\n",
            "                               ...                        \n",
            "39995                                      johnlloydtaylor\n",
            "39996                               happy mothers day love\n",
            "39997    happy mothers day mommies woman man long youre...\n",
            "39998    niariley wassup beautiful follow peep new hit ...\n",
            "39999    mopedronin bullet train tokyo gf visiting japa...\n",
            "Name: content, Length: 36802, dtype: object\n"
          ]
        }
      ],
      "source": [
        "print(data['content'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LyyrPCEit1m"
      },
      "source": [
        "# **Lemmatisation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JZgSz4M1iy47"
      },
      "outputs": [],
      "source": [
        "from textblob import Word\n",
        "\n",
        "data['content'] = data['content'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lzR54RQWiy2b",
        "outputId": "eb3b72f4-23a0-458f-bcaf-a45bfe34a28e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1                   layin n bed headache ughhhhwaitin call\n",
            "2                            funeral ceremonygloomy friday\n",
            "4        dannycastillo want trade someone houston ticke...\n",
            "5        repinging ghostridah14 didnt go prom bc bf did...\n",
            "6        sleep im thinking old friend want he married d...\n",
            "                               ...                        \n",
            "39995                                      johnlloydtaylor\n",
            "39996                                happy mother day love\n",
            "39997    happy mother day mommy woman man long youre mo...\n",
            "39998    niariley wassup beautiful follow peep new hit ...\n",
            "39999    mopedronin bullet train tokyo gf visiting japa...\n",
            "Name: content, Length: 36802, dtype: object\n"
          ]
        }
      ],
      "source": [
        "print(data['content'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CeATtz52mtGG"
      },
      "source": [
        "# **Stemming**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OycYrIqOms0f"
      },
      "outputs": [],
      "source": [
        "ps = PorterStemmer()\n",
        " \n",
        "data['content'] = data['content'].apply(lambda x: \" \".join([ps.stem(word) for word in x.split()]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cn0vsoaTiyzl",
        "outputId": "fa941dc8-0cca-4a99-9189-51e7c964046b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1                    layin n bed headach ughhhhwaitin call\n",
            "2                              funer ceremonygloomi friday\n",
            "4        dannycastillo want trade someon houston ticket...\n",
            "5        reping ghostridah14 didnt go prom bc bf didnt ...\n",
            "6        sleep im think old friend want he marri damn a...\n",
            "                               ...                        \n",
            "39995                                      johnlloydtaylor\n",
            "39996                                happi mother day love\n",
            "39997    happi mother day mommi woman man long your mom...\n",
            "39998    niariley wassup beauti follow peep new hit sin...\n",
            "39999    mopedronin bullet train tokyo gf visit japan s...\n",
            "Name: content, Length: 36802, dtype: object\n"
          ]
        }
      ],
      "source": [
        "print(data['content'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vgivbvb1oOy5"
      },
      "source": [
        "# **Removing 10,000 rare words**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f7DhR5aqiyre"
      },
      "outputs": [],
      "source": [
        "freq = pd.Series(' '.join(data['content']).split()).value_counts()[-10000:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n1TIXk8xo9hN"
      },
      "outputs": [],
      "source": [
        "data['content'] = data['content'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c8kh-waQorDt",
        "outputId": "1d94e905-e627-4466-cd94-6e0293360592"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1                    layin n bed headach ughhhhwaitin call\n",
            "2                              funer ceremonygloomi friday\n",
            "4                     want trade someon houston ticket one\n",
            "5        reping ghostridah14 didnt go prom bc bf didnt ...\n",
            "6        sleep im think old friend want he marri damn a...\n",
            "                               ...                        \n",
            "39995                                      johnlloydtaylor\n",
            "39996                                happi mother day love\n",
            "39997    happi mother day mommi woman man long your mom...\n",
            "39998    niariley wassup beauti follow peep new hit sin...\n",
            "39999    mopedronin bullet train tokyo gf visit japan s...\n",
            "Name: content, Length: 36802, dtype: object\n"
          ]
        }
      ],
      "source": [
        "print(data['content'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lV406Kb3zII"
      },
      "source": [
        "# **Label encoding**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NkbUDm9RpCjF"
      },
      "outputs": [],
      "source": [
        "label_encoding = preprocessing.LabelEncoder()\n",
        "y = label_encoding.fit_transform(data.sentiment.values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q4GI-mqd3O2_",
        "outputId": "9dfa2f32-9b4f-476c-dbb5-abe451853598"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 - negative\n",
            "1 - neutral\n",
            "2 - positive\n"
          ]
        }
      ],
      "source": [
        "for i in range(len(y)):\n",
        "  if y[i] == 0:\n",
        "    print(y[i],'-',data.sentiment.values[i])\n",
        "    break\n",
        "for i in range(len(y)):\n",
        "  if y[i] == 1:\n",
        "    print(y[i],'-',data.sentiment.values[i])\n",
        "    break\n",
        "for i in range(len(y)):\n",
        "  if y[i] == 2:\n",
        "    print(y[i],'-',data.sentiment.values[i])\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUDNahg434JH"
      },
      "source": [
        "# **Train test split**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZK9hA7wqAB6J"
      },
      "outputs": [],
      "source": [
        "Test_size = 3000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MPIgjIZW3Wfm"
      },
      "outputs": [],
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(data.content.values, y, stratify=y, random_state=42, test_size=0.1, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4n9aaL0RUksa"
      },
      "outputs": [],
      "source": [
        "\n",
        "X_train_ = X_train[:Test_size]\n",
        "y_train_ = y_train[:Test_size]\n",
        "\n",
        "X_test = np.append(X_val, X_train_)\n",
        "y_test = np.append(y_val, y_train_)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMlMqMOy5wAH"
      },
      "source": [
        "# **Extracting TF-IDF parameters**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hwMSq-Pn3vYt"
      },
      "outputs": [],
      "source": [
        "tfidf = TfidfVectorizer(max_features=1000, analyzer='word',ngram_range=(1,3))\n",
        "X_train_tfidf = tfidf.fit_transform(X_train)\n",
        "X_val_tfidf = tfidf.fit_transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7T_gB7vG57G0"
      },
      "source": [
        "# **Extracting Count Vectors Parameters**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kSv9eLzB4ZYq"
      },
      "outputs": [],
      "source": [
        "count_vect = CountVectorizer(analyzer='word')\n",
        "count_vect.fit(data['content'])\n",
        "X_train_count =  count_vect.transform(X_train)\n",
        "X_val_count =  count_vect.transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9pcpuKg6M4m"
      },
      "source": [
        "# **Building models using count vectors feature**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c8WJAp7-hvNv",
        "outputId": "2d912815-d1dc-49ee-a116-9f9985e8b4cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using TF-IDF as feature vector, accuracy for\n",
            "naive bayes = 0.40428079628798086\n",
            "Linear SVM = 0.41206406226612785\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "logistic regression = 0.39589881754228406\n",
            "random forest = 0.3955994611585092\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Model 1: Multinomial Naive Bayes Classifier\n",
        "print('Using TF-IDF as feature vector, accuracy for')\n",
        "nb = MultinomialNB()\n",
        "nb.fit(X_train_tfidf, y_train)\n",
        "y_pred = nb.predict(X_val_tfidf)\n",
        "print('naive bayes = %s' % accuracy_score(y_pred, y_test))\n",
        "\n",
        "\n",
        "# Model 2: Linear SVM\n",
        "lsvm = SGDClassifier(alpha=0.001, random_state=5, max_iter=15, tol=None)\n",
        "lsvm.fit(X_train_tfidf, y_train)\n",
        "y_pred = lsvm.predict(X_val_tfidf)\n",
        "print('Linear SVM = %s' % accuracy_score(y_pred, y_test))\n",
        "\n",
        "# Model 3: logistic regression\n",
        "logreg = LogisticRegression(C=1)\n",
        "logreg.fit(X_train_tfidf, y_train)\n",
        "y_pred = logreg.predict(X_val_tfidf)\n",
        "print('logistic regression = %s' % accuracy_score(y_pred, y_test))\n",
        "\n",
        "\n",
        "# Model 4: Random Forest Classifier\n",
        "rf = RandomForestClassifier(n_estimators=100)\n",
        "rf.fit(X_train_tfidf, y_train)\n",
        "y_pred = rf.predict(X_val_tfidf)\n",
        "print('random forest = %s' % accuracy_score(y_pred, y_test))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQ-6WzzN6FeO"
      },
      "source": [
        "# **Building models using count vectors feature**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-nZCFUXX5sfe",
        "outputId": "9fcba122-bf7a-41db-e666-83305f5b1906"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using Count Vectors as feature vector, accuracy for\n",
            "naive bayes = accuracy 0.6561891932345457\n",
            "Linear SVM = accuracy 0.6301451878461308\n",
            "logistic regression = accuracy 0.7088759167789254\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print('Using Count Vectors as feature vector, accuracy for')\n",
        "# Model 1: Multinomial Naive Bayes Classifier\n",
        "nb = MultinomialNB()\n",
        "nb.fit(X_train_count, y_train)\n",
        "y_pred = nb.predict(X_val_count)\n",
        "print('naive bayes = accuracy %s' % accuracy_score(y_pred, y_test))\n",
        "\n",
        "\n",
        "# Model 2: Linear SVM\n",
        "lsvm = SGDClassifier(alpha=0.001, random_state=5, max_iter=15, tol=None)\n",
        "lsvm.fit(X_train_count, y_train)\n",
        "y_pred = lsvm.predict(X_val_count)\n",
        "print('Linear SVM = accuracy %s' % accuracy_score(y_pred, y_test))\n",
        "\n",
        "\n",
        "# Model 3: Logistic Regression\n",
        "logreg = LogisticRegression(C=1)\n",
        "logreg.fit(X_train_count, y_train)\n",
        "y_pred = logreg.predict(X_val_count)\n",
        "print('logistic regression = accuracy %s' % accuracy_score(y_pred, y_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lS7mAyU4umIH"
      },
      "outputs": [],
      "source": [
        "di = {0:\"negative\",1:\"neutral\",2:\"positive\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDsDway56QgP"
      },
      "source": [
        "# **Demo for some random statements**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h_Z33Q3N706v",
        "outputId": "d458f905-3bfd-48fe-df7e-ba7895988802"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2 1 2 0 0 1 2 2 0 1 1 0]\n"
          ]
        }
      ],
      "source": [
        "#Below are some random statements. The first 4 depict happiness. The last 4 depict sadness\n",
        "#0 - negative\n",
        "#1 - neutral\n",
        "#2 - positive\n",
        "tweets = pd.DataFrame(['This is a nice project', \n",
        "                       '', \n",
        "                       'There will be coffee, sweets and dance', \n",
        "                       'Oh, the chocolate will be wonderful', \n",
        "                       'I am so sad as I am missing home', \n",
        "                       'At home alone with not much to do',\n",
        "                       'I want to enjoy with my friends',\n",
        "                       'It will be a lot more fun in college',\n",
        "                       'I will kill you',\n",
        "                       'I am positive',\n",
        "                       'load beard papa disappear uk',\n",
        "                       'I will fail the test'])\n",
        "\n",
        "# Doing some preprocessing on these tweets as done before\n",
        "tweets[0] = tweets[0].str.replace('[^\\w\\s]',' ')\n",
        "from nltk.corpus import stopwords\n",
        "tweets[0] = tweets[0].apply(lambda x: \" \".join(x for x in x.split() if x not in stop_words))\n",
        "from textblob import Word\n",
        "tweets[0] = tweets[0].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
        "\n",
        "# Extracting Count Vectors feature from our tweets\n",
        "tweet_count = count_vect.transform(tweets[0])\n",
        "\n",
        "#Predicting the emotion of the tweet using our already trained logistic regression\n",
        "tweet_pred = logreg.predict(tweet_count)\n",
        "print(tweet_pred)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3jUFNRf6eDM"
      },
      "source": [
        "# **Evaluation metrics**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cGDNQ1ujp1Hu",
        "outputId": "88421be4-86a3-4a65-d0aa-490647711f80"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CONFUSION MATRIX - \n",
            "[[1848  324  362]\n",
            " [ 257  867  255]\n",
            " [ 380  367 2021]]\n",
            "\n",
            "ACCURACY SCORE - 0.7088759167789254\n",
            "\n",
            "CLASSIFICATION REPORT  - \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.74      0.73      0.74      2534\n",
            "           1       0.56      0.63      0.59      1379\n",
            "           2       0.77      0.73      0.75      2768\n",
            "\n",
            "    accuracy                           0.71      6681\n",
            "   macro avg       0.69      0.70      0.69      6681\n",
            "weighted avg       0.71      0.71      0.71      6681\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
        "print(\"CONFUSION MATRIX - \")\n",
        "print(confusion_matrix(y_pred, y_test))\n",
        "print()\n",
        "print(\"ACCURACY SCORE\",\"-\",accuracy_score(y_pred, y_test))\n",
        "print()\n",
        "print(\"CLASSIFICATION REPORT\",\" - \")\n",
        "print(classification_report(y_pred, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d60pye5S6ro7"
      },
      "source": [
        "# **Speech to text**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7SBqvObcy002",
        "outputId": "753a5a2c-2c8e-4495-8a44-a0870e9750ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pydub in /usr/local/lib/python3.7/dist-packages (0.25.1)\n",
            "Requirement already satisfied: SpeechRecognition in /usr/local/lib/python3.7/dist-packages (3.8.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install pydub\n",
        "!pip install SpeechRecognition\n",
        "import speech_recognition as sr\n",
        "import os\n",
        "from pydub import AudioSegment\n",
        "\n",
        "def speech_to_text(path):\n",
        "  r = sr.Recognizer()\n",
        "\n",
        "  sound = AudioSegment.from_ogg(path)\n",
        "  sound.export(\"/content/live_demo.wav\", format=\"wav\")\n",
        "  path = path[:-3] + \"wav\"\n",
        "  with sr.AudioFile(path) as source:\n",
        "    audio = r.listen(source)\n",
        "  \n",
        "  try:\n",
        "    print(r.recognize_google(audio))\n",
        "  except:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kyfmZPruLAbv",
        "outputId": "cc0c5a28-4c10-4df4-8a3f-1e7b38e791a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "this is a nice project\n"
          ]
        }
      ],
      "source": [
        "voice_1 = speech_to_text(\"/content/live_demo.wav\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HH_OuJJv7D7W"
      },
      "source": [
        "# **Speech emotion detection**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_E9lxPWx10F2",
        "outputId": "fd3e282d-8a0b-42a8-f2cc-d7690a34e41c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2 - positive\n"
          ]
        }
      ],
      "source": [
        "tweets = pd.DataFrame([str(voice_1)])\n",
        "# Doing some preprocessing on these tweets as done before\n",
        "tweets[0] = tweets[0].str.replace('[^\\w\\s]',' ')\n",
        "from nltk.corpus import stopwords\n",
        "tweets[0] = tweets[0].apply(lambda x: \" \".join(x for x in x.split() if x not in stop_words))\n",
        "from textblob import Word\n",
        "tweets[0] = tweets[0].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
        "\n",
        "# Extracting Count Vectors feature from our tweets\n",
        "tweet_count = count_vect.transform(tweets[0])\n",
        "\n",
        "#Predicting the emotion of the tweet using our already trained linear SVM\n",
        "tweet_pred = logreg.predict(tweet_count)\n",
        "print(tweet_pred[0],'-',di[tweet_pred[0]])\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Varshith_NLP.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
